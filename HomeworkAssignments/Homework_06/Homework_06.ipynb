{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d9e5bb-5355-4691-bd26-0aade0d24f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optuna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.10.2)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.22.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.4.46)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: colorlog in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b015af1-68ca-4db8-8aac-0273f5a75000",
   "metadata": {},
   "source": [
    "**Exercise 1: (5 points) Using the bucket, that you create in the last homework assignment, and the pandas\n",
    "library, read the train.csv and test.csv data files and create two data-frames called train and\n",
    "test, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15da6ae7-7ca2-4278-afd7-3ab2178d7ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "import cost_functions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier \n",
    "from sklearn.metrics import classification_report, make_scorer, confusion_matrix\n",
    "\n",
    "## Defining the s3 bucket\n",
    "s3= boto3.resource('s3')\n",
    "bucket_name= 'craig-shaffer-data-445-bucket'\n",
    "bucket= s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the file to be read from s3 bucket\n",
    "file_key = 'train.csv'\n",
    "file_key2 = 'test.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object2 = bucket_object2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "## Reading the datafiles\n",
    "train = pd.read_csv(file_content_stream, sep = '|')\n",
    "test = pd.read_csv(file_content_stream2, sep = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f51e7-02ed-444c-9b9f-a56fa7d5e5e1",
   "metadata": {},
   "source": [
    "*Engineering variables from previous homeworks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98f0819-4396-4068-85ba-ae4d5ccf27ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#variable one: low trust level (trustLevel for fraud is never >2)\n",
    "train['lowTrust'] = np.where(train['trustLevel'] <= 2, 1, 0)\n",
    "test['lowTrust'] = np.where(test['trustLevel'] <= 2, 1, 0)\n",
    "\n",
    "#variable two: low value per second (the highest value per second (VPS) in the fraud data set is .231, but will set it to .25)\n",
    "train['lowVPS'] = np.where(train['valuePerSecond'] <= 0.25, 1, 0)\n",
    "test['lowVPS'] = np.where(test['valuePerSecond'] <= 0.25, 1, 0)\n",
    "\n",
    "#variable three: low scan time (noticeable difference in quartiles for fraud and not fraud)\n",
    "train['lowTotalScanTime'] = np.where(train['totalScanTimeInSeconds'] < 1000, 1, 0)\n",
    "test['lowTotalScanTime'] = np.where(test['totalScanTimeInSeconds'] < 1000, 1, 0)\n",
    "\n",
    "#variable four: high scannedLineItemsPerSecond (SLIPS) (SLIPS doesn't exceed .308 in fraud but goes up to 11 in not fraud)\n",
    "train['highSLIPS'] = np.where(train['scannedLineItemsPerSecond'] > 0.35 , 1, 0)\n",
    "test['highSLIPS'] = np.where(test['scannedLineItemsPerSecond'] > 0.35 , 1, 0)\n",
    "\n",
    "#variable five: boxcox transformation on scannedLineItemsPerSecond\n",
    "train['boxcox_SLIPS'] = boxcox(train['scannedLineItemsPerSecond'])[0]\n",
    "test['boxcox_SLIPS'] = boxcox(test['scannedLineItemsPerSecond'])[0]\n",
    "\n",
    "#variable six: 1/grandTotal\n",
    "train['1_grandTotal'] = 1/(train['grandTotal'])\n",
    "test['1_grandTotal'] = 1/(test['grandTotal'])\n",
    "\n",
    "#variable seven: natural log of totalScanTimeInSeconds\n",
    "train['log_totalScanTimeInSeconds']= np.log(train['totalScanTimeInSeconds'])\n",
    "test['log_totalScanTimeInSeconds']= np.log(test['totalScanTimeInSeconds'])\n",
    "\n",
    "#variable eight: lineItemVoidsPerPosition^2\n",
    "train['squared_lineItemVoidsPerPosition']= np.power(train['lineItemVoidsPerPosition'], 2)\n",
    "test['squared_lineItemVoidsPerPosition']= np.power(test['lineItemVoidsPerPosition'], 2)\n",
    "\n",
    "#variable nine: attempted a scan without registration\n",
    "train['madeScansWithoutRegistration'] = np.where(train['scansWithoutRegistration'] > 0, 1, 0)\n",
    "test['madeScansWithoutRegistration'] = np.where(test['scansWithoutRegistration'] > 0, 1, 0)\n",
    "\n",
    "#variable ten: made a modification to quantity\n",
    "train['madeModification'] = np.where(train['quantityModifications'] > 0, 1, 0)\n",
    "test['madeModification'] = np.where(test['quantityModifications'] > 0, 1, 0)\n",
    "\n",
    "#3 heredity principle features\n",
    "train['heredity_interaction_1'] = train['trustLevel'] * train['lowTrust']\n",
    "test['heredity_interaction_1'] = test['trustLevel'] * test['lowTrust']\n",
    "\n",
    "train['heredity_interaction_2'] = train['trustLevel'] * train['scannedLineItemsPerSecond']\n",
    "test['heredity_interaction_2'] = test['trustLevel'] * test['scannedLineItemsPerSecond']\n",
    "\n",
    "train['heredity_interaction_3'] = train['lowTrust'] * train['scannedLineItemsPerSecond']\n",
    "test['heredity_interaction_3'] = test['lowTrust'] * test['scannedLineItemsPerSecond']\n",
    "\n",
    "\n",
    "#decision tree features\n",
    "train['tree_interaction_1'] = np.where(train['heredity_interaction_3'] <= 0.012, 1, 0)\n",
    "test['tree_interaction_1'] = np.where(test['heredity_interaction_3'] <= 0.012, 1, 0)\n",
    "\n",
    "train['tree_interaction_2'] = np.where((train['heredity_interaction_3'] > 0.012) & \n",
    "                                       (train['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (train['heredity_interaction_1'] > 1.5) &\n",
    "                                       (train['scansWithoutRegistration'] <= 7.5), 1, 0)\n",
    "test['tree_interaction_2'] = np.where((test['heredity_interaction_3'] > 0.012) & \n",
    "                                       (test['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (test['heredity_interaction_1'] > 1.5) &\n",
    "                                       (test['scansWithoutRegistration'] <= 7.5), 1, 0)\n",
    "\n",
    "train['tree_interaction_3'] = np.where((train['heredity_interaction_3'] > 0.012) & \n",
    "                                       (train['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (train['heredity_interaction_1'] <= 1.5) &\n",
    "                                       (train['valuePerSecond'] <= 0.119), 1, 0)\n",
    "test['tree_interaction_3'] = np.where((test['heredity_interaction_3'] > 0.012) & \n",
    "                                       (test['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (test['heredity_interaction_1'] <= 1.5) &\n",
    "                                       (test['valuePerSecond'] <= 0.119), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9c11c9-c292-466b-8bd7-7edb6b14c266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining the input (top 7 features) and target variable (fraud)\n",
    "x_train_7 = train[['log_totalScanTimeInSeconds', 'trustLevel', 'tree_interaction_1', 'heredity_interaction_3', \n",
    "                 'boxcox_SLIPS','scansWithoutRegistration','lineItemVoids']]\n",
    "y_train = train['fraud']\n",
    "\n",
    "#top 6 features\n",
    "x_train_6 = x_train_7.drop(columns = ['lineItemVoids'])\n",
    "\n",
    "#top 5 features\n",
    "x_train_5 = x_train_6.drop(columns = ['scansWithoutRegistration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e28fbc-688d-41b1-b0f8-0588a3e0dcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining scorer\n",
    "my_scorer = make_scorer(cost_functions.cost_function, greater_is_better = True, needs_proba = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0e6d6-3bde-4dab-90ec-3eb036997834",
   "metadata": {},
   "source": [
    "**Exercise 2: (85 points) Using the train data-frame (including the top 7 features from homework assignment 5), do the following:**\n",
    "\n",
    "- (i) Consider a model to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "\n",
    "From above three scenarios, identify the best model; that is, the model (input features\n",
    "and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e712ad37-74c5-4b1d-a049-ef7507d70d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for GradientBoostingClassifier with top-5 variables: \n",
      " {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_leaf': 7, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "Best score:\n",
      " -13.333333333333334\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for GradientBoostingClassifier with top-6 variables: \n",
      " {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "Best score:\n",
      " -6.666666666666667\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for GradientBoostingClassifier with top-7 variables: \n",
      " {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_leaf': 7, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "Best score:\n",
      " -13.333333333333334\n"
     ]
    }
   ],
   "source": [
    "#Model: Gradient Boosting\n",
    "\n",
    "#defining parameter dictionary\n",
    "gb_param_grid = {'n_estimators': [100, 300],\n",
    "                  'min_samples_split': [10, 15],\n",
    "                  'min_samples_leaf': [5, 7],\n",
    "                  'max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "#GridSearchCV w/ top 5 most important features:----------\n",
    "gb_grid_search_1 = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = gb_param_grid, cv = 3, \n",
    "                                scoring = my_scorer).fit(x_train_5, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for GradientBoostingClassifier with top-5 variables: \\n', gb_grid_search_1.best_params_)\n",
    "print('\\nBest score:\\n', gb_grid_search_1.best_score_)\n",
    "print('\\n----------')\n",
    "#GridSearchCV w/ top 6 most important features:----------\n",
    "gb_grid_search_2 = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = gb_param_grid, cv = 3, \n",
    "                                scoring = my_scorer).fit(x_train_6, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for GradientBoostingClassifier with top-6 variables: \\n', gb_grid_search_2.best_params_)\n",
    "print('\\nBest score:\\n', gb_grid_search_2.best_score_)\n",
    "print('\\n----------')\n",
    "#GridSearchCV w/ top 7 most important features:----------\n",
    "gb_grid_search_3 = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = gb_param_grid, cv = 3, \n",
    "                                scoring = my_scorer).fit(x_train_5, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for GradientBoostingClassifier with top-7 variables: \\n', gb_grid_search_3.best_params_)\n",
    "print('\\nBest score:\\n', gb_grid_search_3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f621c9-53ad-42e2-95b1-6272969123ac",
   "metadata": {},
   "source": [
    "- (ii) Consider a model different from part (i) to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  \n",
    "From above three scenarios, identify the best model; that is, the model (input features and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996328a2-fdde-4075-9300-02070ee55758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for AdaBoostClassifier with top-5 variables: \n",
      " {'n_estimators': 300, 'learning_rate': 0.01, 'estimator__min_samples_split': 10, 'estimator__min_samples_leaf': 7, 'estimator__max_depth': 3}\n",
      "\n",
      "Best score:\n",
      " -21.666666666666668\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for AdaBoostClassifier with top-6 variables: \n",
      " {'n_estimators': 100, 'learning_rate': 0.01, 'estimator__min_samples_split': 15, 'estimator__min_samples_leaf': 7, 'estimator__max_depth': 3}\n",
      "\n",
      "Best score:\n",
      " 13.333333333333334\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for AdaBoostClassifier with top-7 variables: \n",
      " {'n_estimators': 100, 'learning_rate': 0.01, 'estimator__min_samples_split': 10, 'estimator__min_samples_leaf': 7, 'estimator__max_depth': 3}\n",
      "\n",
      "Best score:\n",
      " -3.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "#Model: AdaBoost\n",
    "\n",
    "#defining parameter dictionary\n",
    "ada_param_grid = {'n_estimators': [100, 300],\n",
    "                  'estimator__min_samples_split': [10, 15],\n",
    "                  'estimator__min_samples_leaf': [5, 7],\n",
    "                  'estimator__max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "#RandomizedSearchCV w/ top 5 most important features:----------\n",
    "ada_randomized_search_1 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_5, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-5 variables: \\n', ada_randomized_search_1.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_1.best_score_)\n",
    "print('\\n----------')\n",
    "#RandomizedSearchCV w/ top 6 most important features:----------\n",
    "ada_randomized_search_2 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_6, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-6 variables: \\n', ada_randomized_search_2.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_2.best_score_)\n",
    "print('\\n----------')\n",
    "#RandomizedSearchCV w/ top 7 most important features:----------\n",
    "ada_randomized_search_3 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_7, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-7 variables: \\n', ada_randomized_search_3.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d193-dfc2-45bd-8032-26ce9c3086e1",
   "metadata": {},
   "source": [
    "- (iii) Consider a model different from parts (i) & (ii) to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "\n",
    "From above three scenarios, identify the best model; that is, the model (input features and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f484c0a4-4d44-47b0-abf2-1cfc9bf06782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-25 00:45:59,483]\u001b[0m A new study created in memory with name: no-name-2f56093e-99e9-463f-b648-85359022a2d9\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:06,594]\u001b[0m Trial 0 finished with value: -51.666666666666664 and parameters: {'n_estimators': 1464, 'min_samples_split': 11, 'min_samples_leaf': 26, 'max_depth': 9}. Best is trial 0 with value: -51.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:15,809]\u001b[0m Trial 1 finished with value: -36.666666666666664 and parameters: {'n_estimators': 1896, 'min_samples_split': 11, 'min_samples_leaf': 12, 'max_depth': 9}. Best is trial 1 with value: -36.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:18,511]\u001b[0m Trial 2 finished with value: -33.333333333333336 and parameters: {'n_estimators': 541, 'min_samples_split': 12, 'min_samples_leaf': 11, 'max_depth': 8}. Best is trial 2 with value: -33.333333333333336.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:27,310]\u001b[0m Trial 3 finished with value: -43.333333333333336 and parameters: {'n_estimators': 1845, 'min_samples_split': 7, 'min_samples_leaf': 22, 'max_depth': 7}. Best is trial 2 with value: -33.333333333333336.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:28,556]\u001b[0m Trial 4 finished with value: -31.666666666666668 and parameters: {'n_estimators': 236, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_depth': 8}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:33,597]\u001b[0m Trial 5 finished with value: -50.0 and parameters: {'n_estimators': 1027, 'min_samples_split': 18, 'min_samples_leaf': 29, 'max_depth': 10}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:40,062]\u001b[0m Trial 6 finished with value: -58.333333333333336 and parameters: {'n_estimators': 1353, 'min_samples_split': 13, 'min_samples_leaf': 25, 'max_depth': 5}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:48,739]\u001b[0m Trial 7 finished with value: -63.333333333333336 and parameters: {'n_estimators': 1833, 'min_samples_split': 16, 'min_samples_leaf': 11, 'max_depth': 3}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:46:57,164]\u001b[0m Trial 8 finished with value: -55.0 and parameters: {'n_estimators': 1770, 'min_samples_split': 26, 'min_samples_leaf': 27, 'max_depth': 8}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:01,452]\u001b[0m Trial 9 finished with value: -38.333333333333336 and parameters: {'n_estimators': 887, 'min_samples_split': 7, 'min_samples_leaf': 22, 'max_depth': 9}. Best is trial 4 with value: -31.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:02,218]\u001b[0m Trial 10 finished with value: -30.0 and parameters: {'n_estimators': 134, 'min_samples_split': 22, 'min_samples_leaf': 6, 'max_depth': 5}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:02,972]\u001b[0m Trial 11 finished with value: -33.333333333333336 and parameters: {'n_estimators': 131, 'min_samples_split': 23, 'min_samples_leaf': 5, 'max_depth': 5}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:03,561]\u001b[0m Trial 12 finished with value: -81.66666666666667 and parameters: {'n_estimators': 101, 'min_samples_split': 21, 'min_samples_leaf': 5, 'max_depth': 2}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:05,977]\u001b[0m Trial 13 finished with value: -48.333333333333336 and parameters: {'n_estimators': 454, 'min_samples_split': 30, 'min_samples_leaf': 16, 'max_depth': 6}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:08,510]\u001b[0m Trial 14 finished with value: -48.333333333333336 and parameters: {'n_estimators': 506, 'min_samples_split': 18, 'min_samples_leaf': 8, 'max_depth': 4}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:10,122]\u001b[0m Trial 15 finished with value: -36.666666666666664 and parameters: {'n_estimators': 311, 'min_samples_split': 5, 'min_samples_leaf': 16, 'max_depth': 6}. Best is trial 10 with value: -30.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:13,839]\u001b[0m Trial 16 finished with value: -26.666666666666668 and parameters: {'n_estimators': 745, 'min_samples_split': 24, 'min_samples_leaf': 8, 'max_depth': 7}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:17,375]\u001b[0m Trial 17 finished with value: -51.666666666666664 and parameters: {'n_estimators': 721, 'min_samples_split': 26, 'min_samples_leaf': 13, 'max_depth': 4}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:23,734]\u001b[0m Trial 18 finished with value: -40.0 and parameters: {'n_estimators': 1294, 'min_samples_split': 22, 'min_samples_leaf': 8, 'max_depth': 7}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:27,608]\u001b[0m Trial 19 finished with value: -33.333333333333336 and parameters: {'n_estimators': 791, 'min_samples_split': 29, 'min_samples_leaf': 19, 'max_depth': 5}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:33,115]\u001b[0m Trial 20 finished with value: -31.666666666666668 and parameters: {'n_estimators': 1119, 'min_samples_split': 25, 'min_samples_leaf': 9, 'max_depth': 7}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:34,644]\u001b[0m Trial 21 finished with value: -31.666666666666668 and parameters: {'n_estimators': 289, 'min_samples_split': 20, 'min_samples_leaf': 6, 'max_depth': 8}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:36,289]\u001b[0m Trial 22 finished with value: -38.333333333333336 and parameters: {'n_estimators': 317, 'min_samples_split': 24, 'min_samples_leaf': 8, 'max_depth': 6}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:39,455]\u001b[0m Trial 23 finished with value: -40.0 and parameters: {'n_estimators': 635, 'min_samples_split': 15, 'min_samples_leaf': 14, 'max_depth': 7}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:40,741]\u001b[0m Trial 24 finished with value: -41.666666666666664 and parameters: {'n_estimators': 242, 'min_samples_split': 28, 'min_samples_leaf': 10, 'max_depth': 10}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:42,897]\u001b[0m Trial 25 finished with value: -35.0 and parameters: {'n_estimators': 425, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_depth': 4}. Best is trial 16 with value: -26.666666666666668.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:47:47,356]\u001b[0m Trial 26 finished with value: -23.333333333333332 and parameters: {'n_estimators': 893, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_depth': 8}. Best is trial 26 with value: -23.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:52,068]\u001b[0m Trial 27 finished with value: -20.0 and parameters: {'n_estimators': 924, 'min_samples_split': 16, 'min_samples_leaf': 5, 'max_depth': 5}. Best is trial 27 with value: -20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:47:57,047]\u001b[0m Trial 28 finished with value: -26.666666666666668 and parameters: {'n_estimators': 999, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_depth': 7}. Best is trial 27 with value: -20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:02,782]\u001b[0m Trial 29 finished with value: -40.0 and parameters: {'n_estimators': 1173, 'min_samples_split': 17, 'min_samples_leaf': 14, 'max_depth': 9}. Best is trial 27 with value: -20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:02,784]\u001b[0m A new study created in memory with name: no-name-5093b142-5cf4-4a08-8a62-97214e13da8d\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for RandomForestClassifier with top-5 variables: \n",
      " {'n_estimators': 924, 'min_samples_split': 16, 'min_samples_leaf': 5, 'max_depth': 5}\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-25 00:48:07,551]\u001b[0m Trial 0 finished with value: -46.666666666666664 and parameters: {'n_estimators': 996, 'min_samples_split': 13, 'min_samples_leaf': 29, 'max_depth': 7}. Best is trial 0 with value: -46.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:13,982]\u001b[0m Trial 1 finished with value: -56.666666666666664 and parameters: {'n_estimators': 1336, 'min_samples_split': 10, 'min_samples_leaf': 26, 'max_depth': 6}. Best is trial 0 with value: -46.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:14,746]\u001b[0m Trial 2 finished with value: -6.666666666666667 and parameters: {'n_estimators': 136, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:24,044]\u001b[0m Trial 3 finished with value: -46.666666666666664 and parameters: {'n_estimators': 1916, 'min_samples_split': 7, 'min_samples_leaf': 18, 'max_depth': 8}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:32,602]\u001b[0m Trial 4 finished with value: -50.0 and parameters: {'n_estimators': 1795, 'min_samples_split': 26, 'min_samples_leaf': 25, 'max_depth': 5}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:41,998]\u001b[0m Trial 5 finished with value: -50.0 and parameters: {'n_estimators': 1963, 'min_samples_split': 21, 'min_samples_leaf': 21, 'max_depth': 4}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:44,128]\u001b[0m Trial 6 finished with value: -43.333333333333336 and parameters: {'n_estimators': 428, 'min_samples_split': 19, 'min_samples_leaf': 14, 'max_depth': 4}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:51,119]\u001b[0m Trial 7 finished with value: -61.666666666666664 and parameters: {'n_estimators': 1474, 'min_samples_split': 13, 'min_samples_leaf': 8, 'max_depth': 3}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:48:58,534]\u001b[0m Trial 8 finished with value: -30.0 and parameters: {'n_estimators': 1527, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_depth': 5}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:03,036]\u001b[0m Trial 9 finished with value: -43.333333333333336 and parameters: {'n_estimators': 884, 'min_samples_split': 22, 'min_samples_leaf': 16, 'max_depth': 4}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:04,254]\u001b[0m Trial 10 finished with value: -18.333333333333332 and parameters: {'n_estimators': 225, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:04,899]\u001b[0m Trial 11 finished with value: -11.666666666666666 and parameters: {'n_estimators': 107, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:07,738]\u001b[0m Trial 12 finished with value: -16.666666666666668 and parameters: {'n_estimators': 565, 'min_samples_split': 6, 'min_samples_leaf': 11, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:08,361]\u001b[0m Trial 13 finished with value: -15.0 and parameters: {'n_estimators': 103, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:49:11,720]\u001b[0m Trial 14 finished with value: -18.333333333333332 and parameters: {'n_estimators': 666, 'min_samples_split': 30, 'min_samples_leaf': 11, 'max_depth': 8}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:49:13,429]\u001b[0m Trial 15 finished with value: -25.0 and parameters: {'n_estimators': 330, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:17,105]\u001b[0m Trial 16 finished with value: -11.666666666666666 and parameters: {'n_estimators': 727, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:49:17,848]\u001b[0m Trial 17 finished with value: -35.0 and parameters: {'n_estimators': 130, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_depth': 8}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:20,156]\u001b[0m Trial 18 finished with value: -50.0 and parameters: {'n_estimators': 460, 'min_samples_split': 15, 'min_samples_leaf': 20, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:49:24,155]\u001b[0m Trial 19 finished with value: -13.333333333333334 and parameters: {'n_estimators': 803, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_depth': 7}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:49:25,785]\u001b[0m Trial 20 finished with value: -26.666666666666668 and parameters: {'n_estimators': 315, 'min_samples_split': 16, 'min_samples_leaf': 11, 'max_depth': 7}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:29,239]\u001b[0m Trial 21 finished with value: -20.0 and parameters: {'n_estimators': 684, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:34,515]\u001b[0m Trial 22 finished with value: -25.0 and parameters: {'n_estimators': 1072, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_depth': 10}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:40,270]\u001b[0m Trial 23 finished with value: -8.333333333333334 and parameters: {'n_estimators': 1165, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:45,851]\u001b[0m Trial 24 finished with value: -16.666666666666668 and parameters: {'n_estimators': 1139, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:52,080]\u001b[0m Trial 25 finished with value: -21.666666666666668 and parameters: {'n_estimators': 1259, 'min_samples_split': 11, 'min_samples_leaf': 13, 'max_depth': 8}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:49:59,983]\u001b[0m Trial 26 finished with value: -78.33333333333333 and parameters: {'n_estimators': 1710, 'min_samples_split': 7, 'min_samples_leaf': 7, 'max_depth': 2}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:04,519]\u001b[0m Trial 27 finished with value: -20.0 and parameters: {'n_estimators': 914, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_depth': 9}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:07,227]\u001b[0m Trial 28 finished with value: -36.666666666666664 and parameters: {'n_estimators': 526, 'min_samples_split': 5, 'min_samples_leaf': 15, 'max_depth': 6}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:12,971]\u001b[0m Trial 29 finished with value: -51.666666666666664 and parameters: {'n_estimators': 1201, 'min_samples_split': 8, 'min_samples_leaf': 28, 'max_depth': 7}. Best is trial 2 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:12,973]\u001b[0m A new study created in memory with name: no-name-15c667a1-21d3-4586-8b2a-7116922809e1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for RandomForestClassifier with top-6 variables: \n",
      " {'n_estimators': 136, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_depth': 9}\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-25 00:50:14,671]\u001b[0m Trial 0 finished with value: -63.333333333333336 and parameters: {'n_estimators': 340, 'min_samples_split': 25, 'min_samples_leaf': 28, 'max_depth': 5}. Best is trial 0 with value: -63.333333333333336.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:19,338]\u001b[0m Trial 1 finished with value: -56.666666666666664 and parameters: {'n_estimators': 977, 'min_samples_split': 13, 'min_samples_leaf': 30, 'max_depth': 6}. Best is trial 1 with value: -56.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:27,848]\u001b[0m Trial 2 finished with value: -40.0 and parameters: {'n_estimators': 1743, 'min_samples_split': 10, 'min_samples_leaf': 15, 'max_depth': 8}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:31,126]\u001b[0m Trial 3 finished with value: -78.33333333333333 and parameters: {'n_estimators': 699, 'min_samples_split': 7, 'min_samples_leaf': 16, 'max_depth': 2}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:32,568]\u001b[0m Trial 4 finished with value: -70.0 and parameters: {'n_estimators': 289, 'min_samples_split': 7, 'min_samples_leaf': 26, 'max_depth': 3}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:35,691]\u001b[0m Trial 5 finished with value: -88.33333333333333 and parameters: {'n_estimators': 666, 'min_samples_split': 9, 'min_samples_leaf': 24, 'max_depth': 2}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:40,846]\u001b[0m Trial 6 finished with value: -46.666666666666664 and parameters: {'n_estimators': 1015, 'min_samples_split': 8, 'min_samples_leaf': 20, 'max_depth': 10}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:42,653]\u001b[0m Trial 7 finished with value: -50.0 and parameters: {'n_estimators': 358, 'min_samples_split': 14, 'min_samples_leaf': 19, 'max_depth': 7}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:48,545]\u001b[0m Trial 8 finished with value: -70.0 and parameters: {'n_estimators': 1241, 'min_samples_split': 27, 'min_samples_leaf': 19, 'max_depth': 3}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:50:51,255]\u001b[0m Trial 9 finished with value: -61.666666666666664 and parameters: {'n_estimators': 554, 'min_samples_split': 17, 'min_samples_leaf': 30, 'max_depth': 4}. Best is trial 2 with value: -40.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:00,926]\u001b[0m Trial 10 finished with value: -18.333333333333332 and parameters: {'n_estimators': 1977, 'min_samples_split': 22, 'min_samples_leaf': 9, 'max_depth': 9}. Best is trial 10 with value: -18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:10,563]\u001b[0m Trial 11 finished with value: -13.333333333333334 and parameters: {'n_estimators': 1929, 'min_samples_split': 22, 'min_samples_leaf': 7, 'max_depth': 9}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:20,368]\u001b[0m Trial 12 finished with value: -21.666666666666668 and parameters: {'n_estimators': 1987, 'min_samples_split': 22, 'min_samples_leaf': 7, 'max_depth': 10}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:28,135]\u001b[0m Trial 13 finished with value: -21.666666666666668 and parameters: {'n_estimators': 1570, 'min_samples_split': 21, 'min_samples_leaf': 5, 'max_depth': 8}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:51:37,949]\u001b[0m Trial 14 finished with value: -30.0 and parameters: {'n_estimators': 1994, 'min_samples_split': 30, 'min_samples_leaf': 11, 'max_depth': 9}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:45,291]\u001b[0m Trial 15 finished with value: -15.0 and parameters: {'n_estimators': 1495, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_depth': 8}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:51:52,143]\u001b[0m Trial 16 finished with value: -28.333333333333332 and parameters: {'n_estimators': 1401, 'min_samples_split': 18, 'min_samples_leaf': 12, 'max_depth': 7}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:51:59,940]\u001b[0m Trial 17 finished with value: -26.666666666666668 and parameters: {'n_estimators': 1601, 'min_samples_split': 19, 'min_samples_leaf': 13, 'max_depth': 8}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:52:08,452]\u001b[0m Trial 18 finished with value: -26.666666666666668 and parameters: {'n_estimators': 1724, 'min_samples_split': 25, 'min_samples_leaf': 9, 'max_depth': 6}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:52:14,921]\u001b[0m Trial 19 finished with value: -13.333333333333334 and parameters: {'n_estimators': 1303, 'min_samples_split': 16, 'min_samples_leaf': 5, 'max_depth': 9}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:52:19,095]\u001b[0m Trial 20 finished with value: -21.666666666666668 and parameters: {'n_estimators': 835, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_depth': 9}. Best is trial 11 with value: -13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:52:25,364]\u001b[0m Trial 21 finished with value: -11.666666666666666 and parameters: {'n_estimators': 1263, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 21 with value: -11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:52:31,444]\u001b[0m Trial 22 finished with value: -11.666666666666666 and parameters: {'n_estimators': 1225, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 21 with value: -11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:52:37,636]\u001b[0m Trial 23 finished with value: -6.666666666666667 and parameters: {'n_estimators': 1251, 'min_samples_split': 13, 'min_samples_leaf': 7, 'max_depth': 10}. Best is trial 23 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:52:43,253]\u001b[0m Trial 24 finished with value: -11.666666666666666 and parameters: {'n_estimators': 1120, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_depth': 10}. Best is trial 23 with value: -6.666666666666667.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:52:49,241]\u001b[0m Trial 25 finished with value: -3.3333333333333335 and parameters: {'n_estimators': 1178, 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_depth': 10}. Best is trial 25 with value: -3.3333333333333335.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:52:53,634]\u001b[0m Trial 26 finished with value: -28.333333333333332 and parameters: {'n_estimators': 888, 'min_samples_split': 5, 'min_samples_leaf': 14, 'max_depth': 10}. Best is trial 25 with value: -3.3333333333333335.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:52:59,185]\u001b[0m Trial 27 finished with value: -16.666666666666668 and parameters: {'n_estimators': 1112, 'min_samples_split': 12, 'min_samples_leaf': 8, 'max_depth': 7}. Best is trial 25 with value: -3.3333333333333335.\u001b[0m\n",
      "\u001b[32m[I 2023-03-25 00:53:05,840]\u001b[0m Trial 28 finished with value: -23.333333333333332 and parameters: {'n_estimators': 1356, 'min_samples_split': 11, 'min_samples_leaf': 11, 'max_depth': 10}. Best is trial 25 with value: -3.3333333333333335.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-25 00:53:11,607]\u001b[0m Trial 29 finished with value: -28.333333333333332 and parameters: {'n_estimators': 1171, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_depth': 5}. Best is trial 25 with value: -3.3333333333333335.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for RandomForestClassifier with top-7 variables: \n",
      " {'n_estimators': 1178, 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "#Model: Random Forest\n",
    "\n",
    "SEED = 42\n",
    "N_TRIALS = 30\n",
    "\n",
    "#Optuna w/ top 5 most important features:----------\n",
    "class Objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 100, 2000), \n",
    "                      min_samples_split = trial.suggest_int('min_samples_split', 5, 30),\n",
    "                      min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 30),\n",
    "                      max_depth = trial.suggest_int('max_depth', 2, 10))\n",
    "        \n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, valid_idx in skf.split(x_train_5, y_train):\n",
    "            \n",
    "            x_train_1, x_valid_1 = x_train_5.iloc[train_idx], x_train_5.iloc[valid_idx]\n",
    "            y_train_1, y_valid_1 = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "            \n",
    "            rf_md = RandomForestClassifier(**params).fit(x_train_1,y_train_1)\n",
    "            \n",
    "            pred_valid = rf_md.predict_proba(x_valid_1)[:,1]\n",
    "            \n",
    "            score = cost_functions.cost_cutoff_function(y_valid_1, pred_valid)\n",
    "            \n",
    "            scores.append(score[0])     \n",
    "            \n",
    "        return np.mean(scores)\n",
    "\n",
    "study_1 = optuna.create_study(direction = 'maximize')\n",
    "study_1.optimize(Objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "print('Best hyper-parameter combination for RandomForestClassifier with top-5 variables: \\n', study_1.best_trial.params)\n",
    "print('\\n----------')\n",
    "#Optuna w/ top 6 most important features:----------\n",
    "class Objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 100, 2000), \n",
    "                      min_samples_split = trial.suggest_int('min_samples_split', 5, 30),\n",
    "                      min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 30),\n",
    "                      max_depth = trial.suggest_int('max_depth', 2, 10))\n",
    "        \n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, valid_idx in skf.split(x_train_6, y_train):\n",
    "            \n",
    "            x_train_1, x_valid_1 = x_train_6.iloc[train_idx], x_train_6.iloc[valid_idx]\n",
    "            y_train_1, y_valid_1 = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "            \n",
    "            rf_md = RandomForestClassifier(**params).fit(x_train_1,y_train_1)\n",
    "            \n",
    "            pred_valid = rf_md.predict_proba(x_valid_1)[:,1]\n",
    "            \n",
    "            score = cost_functions.cost_cutoff_function(y_valid_1, pred_valid)\n",
    "            \n",
    "            scores.append(score[0])     \n",
    "            \n",
    "        return np.mean(scores)\n",
    "\n",
    "study_2 = optuna.create_study(direction = 'maximize')\n",
    "study_2.optimize(Objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "print('Best hyper-parameter combination for RandomForestClassifier with top-6 variables: \\n', study_2.best_trial.params)\n",
    "print('\\n----------')\n",
    "#Optuna w/ top 7 most important features:----------\n",
    "class Objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 100, 2000), \n",
    "                      min_samples_split = trial.suggest_int('min_samples_split', 5, 30),\n",
    "                      min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 30),\n",
    "                      max_depth = trial.suggest_int('max_depth', 2, 10))\n",
    "        \n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, valid_idx in skf.split(x_train_7, y_train):\n",
    "            \n",
    "            x_train_1, x_valid_1 = x_train_7.iloc[train_idx], x_train_7.iloc[valid_idx]\n",
    "            y_train_1, y_valid_1 = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "            \n",
    "            rf_md = RandomForestClassifier(**params).fit(x_train_1,y_train_1)\n",
    "            \n",
    "            pred_valid = rf_md.predict_proba(x_valid_1)[:,1]\n",
    "            \n",
    "            score = cost_functions.cost_cutoff_function(y_valid_1, pred_valid)\n",
    "            \n",
    "            scores.append(score[0])     \n",
    "            \n",
    "        return np.mean(scores)\n",
    "\n",
    "study_3 = optuna.create_study(direction = 'maximize')\n",
    "study_3.optimize(Objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "print('Best hyper-parameter combination for RandomForestClassifier with top-7 variables: \\n', study_3.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa7a9f-5ef6-4c4e-a327-1c4f6b546126",
   "metadata": {},
   "source": [
    "**Exercise 3: (70 points) Using the train data-frame and the models from exercise 2, split the train data-frame into two data-frames: training (80%) and validation (20%) taking into account the proportions of 0s and 1s. Then, do the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fee00af6-24b5-437a-85cb-0aca6555b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input(top 7) and target(fraud)\n",
    "x = train[['log_totalScanTimeInSeconds', 'trustLevel', 'tree_interaction_1', 'heredity_interaction_3', \n",
    "                 'boxcox_SLIPS','scansWithoutRegistration','lineItemVoids']]\n",
    "y = train['fraud']\n",
    "\n",
    "#splitting the data\n",
    "x_training,x_validation,y_training,y_validation = train_test_split(x,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6280caf-5725-4f12-9772-927ea87940e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting top 7 for test\n",
    "test_7 = test[['log_totalScanTimeInSeconds', 'trustLevel', 'tree_interaction_1', 'heredity_interaction_3', \n",
    "                 'boxcox_SLIPS','scansWithoutRegistration','lineItemVoids']]\n",
    "\n",
    "#top 6 variables (for best AdaBoost and GradientBoost models)\n",
    "x_training_6 = x_training.drop(columns = ['lineItemVoids'])\n",
    "x_validation_6 = x_validation.drop(columns = ['lineItemVoids'])\n",
    "test_6 = test_7.drop(columns = ['lineItemVoids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcadd8-93c3-4617-906f-29eec3327f91",
   "metadata": {},
   "source": [
    "- (i) Consider the best model from exercise 2(i). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa21f032-c6bd-4f01-8b0f-325607b2fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function score of GradientBoosting model: -15.0\n",
      "Cutoff value for GradientBoosting model: 0.05\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best hyper-parameter combination for GradientBoostingClassifier with top-6 variables: \n",
    " {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "'''\n",
    "\n",
    "#building the GradientBoosting model\n",
    "gb_md = GradientBoostingClassifier(n_estimators = 300, max_depth = 7, learning_rate = 0.1,\n",
    "                                   min_samples_leaf = 5, min_samples_split = 10).fit(x_training_6, y_training)\n",
    "#predicting on validation & test\n",
    "gb_val_pred = gb_md.predict_proba(x_validation_6)[:, 1]\n",
    "gb_test_pred = gb_md.predict_proba(test_6)[:, 1]\n",
    "\n",
    "#computing the cost for the validation predictions\n",
    "print('Cost function score of GradientBoosting model:', cost_functions.cost_function(y_validation, gb_val_pred))\n",
    "print('Cutoff value for GradientBoosting model:', cost_functions.cost_cutoff_function(y_validation, gb_val_pred)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee71a78-75a7-4b45-bbf7-8aea28b81b4b",
   "metadata": {},
   "source": [
    "- (ii) Consider the best model from exercise 2(ii). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a608b93-8e23-486b-8adc-c32db1e75f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function score of AdaBoost model: 10.0\n",
      "Cutoff value for AdaBoost model: 0.51\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best hyper-parameter combination for AdaBoostClassifier with top-6 variables: \n",
    " {'n_estimators': 100, 'learning_rate': 0.01, 'estimator__min_samples_split': 15, 'estimator__min_samples_leaf': 7, 'estimator__max_depth': 3}\n",
    "'''\n",
    "\n",
    "#building the AdaBoost model\n",
    "ada_md = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 7, min_samples_split = 15),\n",
    "                        learning_rate = 0.01, n_estimators = 100).fit(x_training_6, y_training)\n",
    "#predicting on validation & test\n",
    "ada_val_pred = ada_md.predict_proba(x_validation_6)[:, 1]\n",
    "ada_test_pred = ada_md.predict_proba(test_6)[:, 1]\n",
    "\n",
    "#computing the cost for the validation predictions\n",
    "print('Cost function score of AdaBoost model:', cost_functions.cost_function(y_validation, ada_val_pred))\n",
    "print('Cutoff value for AdaBoost model:', cost_functions.cost_cutoff_function(y_validation, ada_val_pred)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969d52c-530c-41e9-8c25-0013c15210a7",
   "metadata": {},
   "source": [
    "- (iii) Consider the best model from exercise 2(iii). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70419494-905d-4b39-8754-021f6c810d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function score of RandomForest model: 5.0\n",
      "Cutoff value for RandomForest model: 0.56\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best hyper-parameter combination for RandomForestClassifier with top-7 variables: \n",
    " {'n_estimators': 1178, 'min_samples_split': 11, 'min_samples_leaf': 7, 'max_depth': 10}\n",
    "'''\n",
    "\n",
    "#building the GradientBoosting model\n",
    "rf_md = RandomForestClassifier(max_depth = 10, min_samples_leaf = 7, min_samples_split = 11, n_estimators = 1178).fit(x_training, y_training)\n",
    "\n",
    "#predicting on validation & test\n",
    "rf_val_pred = rf_md.predict_proba(x_validation)[:, 1]\n",
    "rf_test_pred = rf_md.predict_proba(test_7)[:, 1]\n",
    "\n",
    "#computing the cost for the validation predictions\n",
    "print('Cost function score of RandomForest model:', cost_functions.cost_function(y_validation, rf_val_pred))\n",
    "print('Cutoff value for RandomForest model:', cost_functions.cost_cutoff_function(y_validation, rf_val_pred)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c468797-045f-48dd-b295-3514b5b8cb2e",
   "metadata": {},
   "source": [
    "Using the prediction on the validation data-frame as inputs from parts (i)-(ii)-(iii) and the actual fraud values from the validation data-frame as the target variable, build a meta-learner to predict fraud. Make sure to tune the hyper-parameters of the meta-learner keeping in mind how the results are going to be evaluated. For more info, see page 4 of DATA-MINING-CUP-2019-task.pdf file. Finally, use the best meta-learner to predict the likelihood of fraud in the test data-frame. Submit the likelihoods in a csv file. Also submit the associated cut-off value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51ebfc42-0a45-4569-9b5d-78a6fee1f234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-26 01:35:03,576]\u001b[0m A new study created in memory with name: no-name-b32402e7-e4b6-49a6-b4c6-cb8cd52ab2e8\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:09,062]\u001b[0m Trial 0 finished with value: -28.333333333333332 and parameters: {'n_estimators': 1304, 'min_samples_split': 17, 'min_samples_leaf': 21, 'max_depth': 10}. Best is trial 0 with value: -28.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:14,295]\u001b[0m Trial 1 finished with value: -6.666666666666667 and parameters: {'n_estimators': 1255, 'min_samples_split': 18, 'min_samples_leaf': 15, 'max_depth': 4}. Best is trial 1 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:15,660]\u001b[0m Trial 2 finished with value: -28.333333333333332 and parameters: {'n_estimators': 312, 'min_samples_split': 11, 'min_samples_leaf': 22, 'max_depth': 4}. Best is trial 1 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:17,144]\u001b[0m Trial 3 finished with value: -28.333333333333332 and parameters: {'n_estimators': 342, 'min_samples_split': 11, 'min_samples_leaf': 27, 'max_depth': 8}. Best is trial 1 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:23,722]\u001b[0m Trial 4 finished with value: -35.0 and parameters: {'n_estimators': 1596, 'min_samples_split': 27, 'min_samples_leaf': 30, 'max_depth': 3}. Best is trial 1 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:30,831]\u001b[0m Trial 5 finished with value: -6.666666666666667 and parameters: {'n_estimators': 1717, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_depth': 2}. Best is trial 1 with value: -6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:35,053]\u001b[0m Trial 6 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 1009, 'min_samples_split': 28, 'min_samples_leaf': 11, 'max_depth': 6}. Best is trial 6 with value: 1.6666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:40,000]\u001b[0m Trial 7 finished with value: -31.666666666666668 and parameters: {'n_estimators': 1192, 'min_samples_split': 18, 'min_samples_leaf': 26, 'max_depth': 8}. Best is trial 6 with value: 1.6666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:42,965]\u001b[0m Trial 8 finished with value: -28.333333333333332 and parameters: {'n_estimators': 702, 'min_samples_split': 13, 'min_samples_leaf': 22, 'max_depth': 7}. Best is trial 6 with value: 1.6666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:46,063]\u001b[0m Trial 9 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 731, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_depth': 2}. Best is trial 6 with value: 1.6666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:35:54,528]\u001b[0m Trial 10 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 1996, 'min_samples_split': 30, 'min_samples_leaf': 8, 'max_depth': 6}. Best is trial 10 with value: 3.3333333333333335.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-26 01:35:58,186]\u001b[0m Trial 11 finished with value: 6.666666666666667 and parameters: {'n_estimators': 858, 'min_samples_split': 29, 'min_samples_leaf': 5, 'max_depth': 6}. Best is trial 11 with value: 6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:06,329]\u001b[0m Trial 12 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 1944, 'min_samples_split': 30, 'min_samples_leaf': 6, 'max_depth': 6}. Best is trial 11 with value: 6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:09,198]\u001b[0m Trial 13 finished with value: 11.666666666666666 and parameters: {'n_estimators': 662, 'min_samples_split': 26, 'min_samples_leaf': 5, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:12,282]\u001b[0m Trial 14 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 694, 'min_samples_split': 24, 'min_samples_leaf': 5, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-26 01:36:16,171]\u001b[0m Trial 15 finished with value: -6.666666666666667 and parameters: {'n_estimators': 922, 'min_samples_split': 24, 'min_samples_leaf': 12, 'max_depth': 4}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:18,280]\u001b[0m Trial 16 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 484, 'min_samples_split': 24, 'min_samples_leaf': 5, 'max_depth': 8}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:20,433]\u001b[0m Trial 17 finished with value: -6.666666666666667 and parameters: {'n_estimators': 498, 'min_samples_split': 26, 'min_samples_leaf': 12, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:24,097]\u001b[0m Trial 18 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 864, 'min_samples_split': 21, 'min_samples_leaf': 8, 'max_depth': 7}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:24,869]\u001b[0m Trial 19 finished with value: -20.0 and parameters: {'n_estimators': 161, 'min_samples_split': 22, 'min_samples_leaf': 18, 'max_depth': 10}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:31,138]\u001b[0m Trial 20 finished with value: -6.666666666666667 and parameters: {'n_estimators': 1511, 'min_samples_split': 28, 'min_samples_leaf': 15, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:39,394]\u001b[0m Trial 21 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 1953, 'min_samples_split': 30, 'min_samples_leaf': 8, 'max_depth': 6}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:36:44,120]\u001b[0m Trial 22 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 1126, 'min_samples_split': 30, 'min_samples_leaf': 7, 'max_depth': 7}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-26 01:36:49,953]\u001b[0m Trial 23 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 1405, 'min_samples_split': 26, 'min_samples_leaf': 10, 'max_depth': 6}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-26 01:36:52,489]\u001b[0m Trial 24 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 593, 'min_samples_split': 28, 'min_samples_leaf': 7, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-26 01:36:56,086]\u001b[0m Trial 25 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 852, 'min_samples_split': 25, 'min_samples_leaf': 5, 'max_depth': 3}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:37:00,425]\u001b[0m Trial 26 finished with value: -6.666666666666667 and parameters: {'n_estimators': 1062, 'min_samples_split': 29, 'min_samples_leaf': 13, 'max_depth': 9}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:37:07,903]\u001b[0m Trial 27 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 1804, 'min_samples_split': 23, 'min_samples_leaf': 9, 'max_depth': 7}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:37:12,039]\u001b[0m Trial 28 finished with value: 3.3333333333333335 and parameters: {'n_estimators': 947, 'min_samples_split': 26, 'min_samples_leaf': 7, 'max_depth': 6}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-03-26 01:37:17,736]\u001b[0m Trial 29 finished with value: -20.0 and parameters: {'n_estimators': 1389, 'min_samples_split': 20, 'min_samples_leaf': 18, 'max_depth': 5}. Best is trial 13 with value: 11.666666666666666.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x_ensemble = pd.concat([pd.DataFrame(gb_val_pred),pd.DataFrame(ada_val_pred),pd.DataFrame(rf_val_pred)], axis = 1)\n",
    "x_test_ensemble = pd.concat([pd.DataFrame(gb_test_pred),pd.DataFrame(ada_test_pred),pd.DataFrame(rf_test_pred)], axis = 1)\n",
    "\n",
    "x = x_ensemble\n",
    "y = y_validation\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self, seed):\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 100, 2000), \n",
    "                      min_samples_split = trial.suggest_int('min_samples_split', 5, 30),\n",
    "                      min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 30),\n",
    "                      max_depth = trial.suggest_int('max_depth', 2, 10))\n",
    "        \n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, valid_idx in skf.split(x, y):\n",
    "            \n",
    "            x_train, x_valid = x.iloc[train_idx], x.iloc[valid_idx]\n",
    "            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "            \n",
    "            rf_md = RandomForestClassifier(**params).fit(x_train,y_train)\n",
    "            \n",
    "            pred_valid = rf_md.predict_proba(x_valid)[:,1]\n",
    "            \n",
    "            score = cost_functions.cost_cutoff_function(y_valid, pred_valid)\n",
    "            \n",
    "            scores.append(score[0])     \n",
    "            \n",
    "        return np.mean(scores)\n",
    "\n",
    "#defining seed and number of trials\n",
    "SEED = 42\n",
    "N_TRIALS = 30\n",
    "\n",
    "#execute an optimization\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(Objective(SEED), n_trials = N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3582d6d7-1c68-4082-afd4-9c8f850d7326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for ensemble model: \n",
      " {'n_estimators': 662, 'min_samples_split': 26, 'min_samples_leaf': 5, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print('Best hyper-parameter combination for ensemble model: \\n', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55a020b8-3f26-4ad0-830d-47e8f39bcd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal cutoff for the ensemble  is: 0.72\n"
     ]
    }
   ],
   "source": [
    "#building ensemble model with optimal hyperparameters (fitted on predictions from validation)\n",
    "rf_ens_md =RandomForestClassifier(**study.best_trial.params).fit(x_ensemble, y_validation)\n",
    "\n",
    "#predicting on validation & test\n",
    "rf_ens_val_pred = rf_ens_md.predict_proba(x_ensemble)[:, 1]\n",
    "rf_ens_test_pred = rf_ens_md.predict_proba(x_test_ensemble)[:, 1]\n",
    "\n",
    "#identifying the optimal cutoff\n",
    "opt_cutoff = cost_functions.cost_cutoff_function(y_validation, rf_ens_val_pred)[1]\n",
    "print('The optimal cutoff for the ensemble  is:', opt_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e69ef421-8bc0-4f93-9e15-07c7b9e69985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#exporting the test predictions as a csv file\n",
    "likelihoods = pd.DataFrame({'Likelihoods': rf_ens_test_pred})\n",
    "likelihoods.to_csv('likelihoods.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a1722-8562-4a53-9af5-8a53ca3dcc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
