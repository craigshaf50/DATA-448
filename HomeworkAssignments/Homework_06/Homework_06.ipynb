{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d9e5bb-5355-4691-bd26-0aade0d24f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (4.64.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.4.46)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (21.3)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from optuna) (5.4.1)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.10.2 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b015af1-68ca-4db8-8aac-0273f5a75000",
   "metadata": {},
   "source": [
    "**Exercise 1: (5 points) Using the bucket, that you create in the last homework assignment, and the pandas\n",
    "library, read the train.csv and test.csv data files and create two data-frames called train and\n",
    "test, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15da6ae7-7ca2-4278-afd7-3ab2178d7ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "from precision_recall_cutoff import precision_recall_cutoff\n",
    "import cost_functions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier \n",
    "from sklearn.metrics import classification_report, make_scorer, confusion_matrix\n",
    "\n",
    "## Defining the s3 bucket\n",
    "s3= boto3.resource('s3')\n",
    "bucket_name= 'craig-shaffer-data-445-bucket'\n",
    "bucket= s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the file to be read from s3 bucket\n",
    "file_key = 'train.csv'\n",
    "file_key2 = 'test.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object2 = bucket_object2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "## Reading the datafiles\n",
    "train = pd.read_csv(file_content_stream, sep = '|')\n",
    "test = pd.read_csv(file_content_stream2, sep = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f51e7-02ed-444c-9b9f-a56fa7d5e5e1",
   "metadata": {},
   "source": [
    "*Engineering variables from previous homeworks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98f0819-4396-4068-85ba-ae4d5ccf27ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#variable one: low trust level (trustLevel for fraud is never >2)\n",
    "train['lowTrust'] = np.where(train['trustLevel'] <= 2, 1, 0)\n",
    "test['lowTrust'] = np.where(test['trustLevel'] <= 2, 1, 0)\n",
    "\n",
    "#variable two: low value per second (the highest value per second (VPS) in the fraud data set is .231, but will set it to .25)\n",
    "train['lowVPS'] = np.where(train['valuePerSecond'] <= 0.25, 1, 0)\n",
    "test['lowVPS'] = np.where(test['valuePerSecond'] <= 0.25, 1, 0)\n",
    "\n",
    "#variable three: low scan time (noticeable difference in quartiles for fraud and not fraud)\n",
    "train['lowTotalScanTime'] = np.where(train['totalScanTimeInSeconds'] < 1000, 1, 0)\n",
    "test['lowTotalScanTime'] = np.where(test['totalScanTimeInSeconds'] < 1000, 1, 0)\n",
    "\n",
    "#variable four: high scannedLineItemsPerSecond (SLIPS) (SLIPS doesn't exceed .308 in fraud but goes up to 11 in not fraud)\n",
    "train['highSLIPS'] = np.where(train['scannedLineItemsPerSecond'] > 0.35 , 1, 0)\n",
    "test['highSLIPS'] = np.where(test['scannedLineItemsPerSecond'] > 0.35 , 1, 0)\n",
    "\n",
    "#variable five: boxcox transformation on scannedLineItemsPerSecond\n",
    "train['boxcox_SLIPS'] = boxcox(train['scannedLineItemsPerSecond'])[0]\n",
    "test['boxcox_SLIPS'] = boxcox(test['scannedLineItemsPerSecond'])[0]\n",
    "\n",
    "#variable six: 1/grandTotal\n",
    "train['1_grandTotal'] = 1/(train['grandTotal'])\n",
    "test['1_grandTotal'] = 1/(test['grandTotal'])\n",
    "\n",
    "#variable seven: natural log of totalScanTimeInSeconds\n",
    "train['log_totalScanTimeInSeconds']= np.log(train['totalScanTimeInSeconds'])\n",
    "test['log_totalScanTimeInSeconds']= np.log(test['totalScanTimeInSeconds'])\n",
    "\n",
    "#variable eight: lineItemVoidsPerPosition^2\n",
    "train['squared_lineItemVoidsPerPosition']= np.power(train['lineItemVoidsPerPosition'], 2)\n",
    "test['squared_lineItemVoidsPerPosition']= np.power(test['lineItemVoidsPerPosition'], 2)\n",
    "\n",
    "#variable nine: attempted a scan without registration\n",
    "train['madeScansWithoutRegistration'] = np.where(train['scansWithoutRegistration'] > 0, 1, 0)\n",
    "test['madeScansWithoutRegistration'] = np.where(test['scansWithoutRegistration'] > 0, 1, 0)\n",
    "\n",
    "#variable ten: made a modification to quantity\n",
    "train['madeModification'] = np.where(train['quantityModifications'] > 0, 1, 0)\n",
    "test['madeModification'] = np.where(test['quantityModifications'] > 0, 1, 0)\n",
    "\n",
    "#3 heredity principle features\n",
    "train['heredity_interaction_1'] = train['trustLevel'] * train['lowTrust']\n",
    "test['heredity_interaction_1'] = test['trustLevel'] * test['lowTrust']\n",
    "\n",
    "train['heredity_interaction_2'] = train['trustLevel'] * train['scannedLineItemsPerSecond']\n",
    "test['heredity_interaction_2'] = test['trustLevel'] * test['scannedLineItemsPerSecond']\n",
    "\n",
    "train['heredity_interaction_3'] = train['lowTrust'] * train['scannedLineItemsPerSecond']\n",
    "test['heredity_interaction_3'] = test['lowTrust'] * test['scannedLineItemsPerSecond']\n",
    "\n",
    "\n",
    "#decision tree features\n",
    "train['tree_interaction_1'] = np.where(train['heredity_interaction_3'] <= 0.012, 1, 0)\n",
    "test['tree_interaction_1'] = np.where(test['heredity_interaction_3'] <= 0.012, 1, 0)\n",
    "\n",
    "train['tree_interaction_2'] = np.where((train['heredity_interaction_3'] > 0.012) & \n",
    "                                       (train['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (train['heredity_interaction_1'] > 1.5) &\n",
    "                                       (train['scansWithoutRegistration'] <= 7.5), 1, 0)\n",
    "test['tree_interaction_2'] = np.where((test['heredity_interaction_3'] > 0.012) & \n",
    "                                       (test['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (test['heredity_interaction_1'] > 1.5) &\n",
    "                                       (test['scansWithoutRegistration'] <= 7.5), 1, 0)\n",
    "\n",
    "train['tree_interaction_3'] = np.where((train['heredity_interaction_3'] > 0.012) & \n",
    "                                       (train['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (train['heredity_interaction_1'] <= 1.5) &\n",
    "                                       (train['valuePerSecond'] <= 0.119), 1, 0)\n",
    "test['tree_interaction_3'] = np.where((test['heredity_interaction_3'] > 0.012) & \n",
    "                                       (test['totalScanTimeInSeconds'] <= 993.0) &\n",
    "                                       (test['heredity_interaction_1'] <= 1.5) &\n",
    "                                       (test['valuePerSecond'] <= 0.119), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9c11c9-c292-466b-8bd7-7edb6b14c266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining the input (top 7 features) and target variable (fraud)\n",
    "x_train_7 = train[['log_totalScanTimeInSeconds', 'trustLevel', 'tree_interaction_1', 'heredity_interaction_3', \n",
    "                 'boxcox_SLIPS','scansWithoutRegistration','lineItemVoids']]\n",
    "y_train = train['fraud']\n",
    "\n",
    "#top 6 features\n",
    "x_train_6 = x_train_7.drop(columns = ['lineItemVoids'])\n",
    "\n",
    "#top 5 features\n",
    "x_train_5 = x_train_6.drop(columns = ['scansWithoutRegistration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e28fbc-688d-41b1-b0f8-0588a3e0dcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining scorer\n",
    "my_scorer = make_scorer(cost_functions.cost_function, greater_is_better = True, needs_proba = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0e6d6-3bde-4dab-90ec-3eb036997834",
   "metadata": {},
   "source": [
    "**Exercise 2: (85 points) Using the train data-frame (including the top 7 features from homework assignment 5), do the following:**\n",
    "\n",
    "- (i) Consider a model to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the GridSearchCV function with cv = 3, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "\n",
    "From above three scenarios, identify the best model; that is, the model (input features\n",
    "and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e712ad37-74c5-4b1d-a049-ef7507d70d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for RandomForestClassifier with top-5 variables: \n",
      " {'max_depth': 7, 'min_samples_leaf': 7, 'min_samples_split': 15, 'n_estimators': 100}\n",
      "\n",
      "Best score:\n",
      " -26.666666666666668\n",
      "\n",
      "----------\n",
      "\n",
      "Best hyper-parameter combination for RandomForestClassifier with top-6 variables: \n",
      " {'max_depth': 7, 'min_samples_leaf': 7, 'min_samples_split': 15, 'n_estimators': 100}\n",
      "\n",
      "Best score:\n",
      " -18.333333333333332\n",
      "\n",
      "----------\n",
      "\n",
      "Best hyper-parameter combination for RandomForestClassifier with top-7 variables: \n",
      " {'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "Best score:\n",
      " -30.0\n"
     ]
    }
   ],
   "source": [
    "#Model: Random Forest\n",
    "\n",
    "#defining parameter dictionary\n",
    "rf_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                 'min_samples_split': [10, 15],\n",
    "                 'min_samples_leaf': [5, 7],\n",
    "                 'max_depth' : [3, 5, 7]}\n",
    "\n",
    "#GridSearchCV w/ top 5 most important features:----------\n",
    "rf_grid_search_1 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = rf_param_grid, \n",
    "                              cv = 3, scoring = my_scorer).fit(x_train_5, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for RandomForestClassifier with top-5 variables: \\n', rf_grid_search_1.best_params_)\n",
    "print('\\nBest score:\\n', rf_grid_search_1.best_score_)\n",
    "print('\\n----------')\n",
    "#GridSearchCV w/ top 6 most important features:----------\n",
    "rf_grid_search_2 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = rf_param_grid, \n",
    "                              cv = 3, scoring = my_scorer).fit(x_train_6, y_train)\n",
    "\n",
    "print('\\nBest hyper-parameter combination for RandomForestClassifier with top-6 variables: \\n', rf_grid_search_2.best_params_)\n",
    "print('\\nBest score:\\n', rf_grid_search_2.best_score_)\n",
    "print('\\n----------')\n",
    "#GridSearchCV w/ top 7 most important features:----------\n",
    "rf_grid_search_3 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = rf_param_grid, \n",
    "                              cv = 3, scoring = my_scorer).fit(x_train_7, y_train)\n",
    "\n",
    "print('\\nBest hyper-parameter combination for RandomForestClassifier with top-7 variables: \\n', rf_grid_search_3.best_params_)\n",
    "print('\\nBest score:\\n', rf_grid_search_3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f621c9-53ad-42e2-95b1-6272969123ac",
   "metadata": {},
   "source": [
    "- (ii) Consider a model different from part (i) to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the RandomizedSearchCV function with cv = 3 and n iter = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  \n",
    "From above three scenarios, identify the best model; that is, the model (input features and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996328a2-fdde-4075-9300-02070ee55758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameter combination for AdaBoostClassifier with top-5 variables: \n",
      " {'n_estimators': 100, 'learning_rate': 0.001, 'estimator__min_samples_split': 10, 'estimator__min_samples_leaf': 5, 'estimator__max_depth': 5}\n",
      "\n",
      "Best score:\n",
      " -18.333333333333332\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for AdaBoostClassifier with top-6 variables: \n",
      " {'n_estimators': 300, 'learning_rate': 0.1, 'estimator__min_samples_split': 15, 'estimator__min_samples_leaf': 5, 'estimator__max_depth': 7}\n",
      "\n",
      "Best score:\n",
      " 1.6666666666666667\n",
      "\n",
      "----------\n",
      "Best hyper-parameter combination for AdaBoostClassifier with top-7 variables: \n",
      " {'n_estimators': 100, 'learning_rate': 0.01, 'estimator__min_samples_split': 15, 'estimator__min_samples_leaf': 7, 'estimator__max_depth': 3}\n",
      "\n",
      "Best score:\n",
      " -3.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "#Model: AdaBoost\n",
    "\n",
    "#defining parameter dictionary\n",
    "ada_param_grid = {'n_estimators': [100, 300],\n",
    "                  'estimator__min_samples_split': [10, 15],\n",
    "                  'estimator__min_samples_leaf': [5, 7],\n",
    "                  'estimator__max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "#RandomizedSearchCV w/ top 5 most important features:----------\n",
    "ada_randomized_search_1 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_5, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-5 variables: \\n', ada_randomized_search_1.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_1.best_score_)\n",
    "print('\\n----------')\n",
    "#RandomizedSearchCV w/ top 6 most important features:----------\n",
    "ada_randomized_search_2 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_6, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-6 variables: \\n', ada_randomized_search_2.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_2.best_score_)\n",
    "print('\\n----------')\n",
    "#RandomizedSearchCV w/ top 7 most important features:----------\n",
    "ada_randomized_search_3 = RandomizedSearchCV(estimator = AdaBoostClassifier(estimator = DecisionTreeClassifier()), \n",
    "                                             param_distributions = ada_param_grid, cv = 3, scoring = my_scorer,\n",
    "                                             n_jobs = -1, n_iter = 30).fit(x_train_7, y_train)\n",
    "\n",
    "print('Best hyper-parameter combination for AdaBoostClassifier with top-7 variables: \\n', ada_randomized_search_3.best_params_)\n",
    "print('\\nBest score:\\n', ada_randomized_search_3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d193-dfc2-45bd-8032-26ce9c3086e1",
   "metadata": {},
   "source": [
    "- (iii) Consider a model different from parts (i) & (ii) to predict fraud. Then, do the following:\n",
    "  - With the top 5 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 6 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "  - With the top 7 important features and using the Optuna framework using 3 folds and N TRIALS = 30, run a hyper-parameter tuning procedure on the model. Please see page 4 of DATA-MINING-CUP-2019-task.pdf file to understand how the model should be evaluated.\n",
    "\n",
    "From above three scenarios, identify the best model; that is, the model (input features and hyper-parameters) that has the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484c0a4-4d44-47b0-abf2-1cfc9bf06782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model: Gradient Boosting\n",
    "\n",
    "#defining parameter dictionary\n",
    "gb_param_grid = {'n_estimators': [100, 300],\n",
    "                  'estimator__min_samples_split': [10, 15],\n",
    "                  'estimator__min_samples_leaf': [5, 7],\n",
    "                  'estimator__max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.01]}\n",
    "\n",
    "#Optuna w/ top 5 most important features:----------\n",
    "\n",
    "#Optuna w/ top 6 most important features:----------\n",
    "\n",
    "#Optuna w/ top 7 most important features:----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa7a9f-5ef6-4c4e-a327-1c4f6b546126",
   "metadata": {},
   "source": [
    "**Exercise 3: (70 points) Using the train data-frame and the models from exercise 2, split the train data-frame into two data-frames: training (80%) and validation (20%) taking into account the proportions of 0s and 1s. Then, do the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee00af6-24b5-437a-85cb-0aca6555b8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dcadd8-93c3-4617-906f-29eec3327f91",
   "metadata": {},
   "source": [
    "- (i) Consider the best model from exercise 2(i). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21f032-c6bd-4f01-8b0f-325607b2fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee71a78-75a7-4b45-bbf7-8aea28b81b4b",
   "metadata": {},
   "source": [
    "- (ii) Consider the best model from exercise 2(ii). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a608b93-8e23-486b-8adc-c32db1e75f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5969d52c-530c-41e9-8c25-0013c15210a7",
   "metadata": {},
   "source": [
    "- (iii) Consider the best model from exercise 2(iii). Build that model on the training data-frame. After that, predict the likelihood of fraud on the validation and test data-frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70419494-905d-4b39-8754-021f6c810d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c468797-045f-48dd-b295-3514b5b8cb2e",
   "metadata": {},
   "source": [
    "Using the prediction on the validation data-frame as inputs from parts (i)-(ii)-(iii) and the actual fraud values from the validation data-frame as the target variable, build a meta-learner to predict fraud. Make sure to tune the hyper-parameters of the meta-learner keeping in mind how the results are going to be evaluated. For more info, see page 4 of DATA-MINING-CUP-2019-task.pdf file. Finally, use the best meta-learner to predict the likelihood of fraud in the test data-frame. Submit the likelihoods in a csv file. Also submit the associated cut-off value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebfc42-0a45-4569-9b5d-78a6fee1f234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
